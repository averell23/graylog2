# If you are running more than one instances of graylog2-server you have to select one of these
# instances as master. The master will perform some periodical tasks that non-masters won't perform.
is_master = true

# Set plugin directory here (relative or absolute)
plugin_dir = plugin

# On which port (UDP) should we listen for Syslog messages? (Standard: 514)
syslog_listen_port = <%= node.graylog2.port %>
syslog_listen_address = 0.0.0.0
syslog_enable_udp = true
syslog_enable_tcp = false
# Standard delimiter is LF. You can force using a NUL byte delimiter using this option.
syslog_use_nul_delimiter = false
# The raw syslog message is stored as full_message of the message if not disabled here.
syslog_store_full_message = true

# Socket receive buffer size (bytes) for UDP syslog and UDP GELF.
udp_recvbuffer_sizes = 1048576

# Embedded elasticsearch configuration file
# pay attention to the working directory of the server, maybe use an absolute path here
elasticsearch_config_file = /etc/graylog2-elasticsearch.yml
elasticsearch_max_docs_per_index = <%= node.graylog2.max_docs_per_index %>

elasticsearch_index_prefix = graylog2

# How many indices do you want to keep? If the number of indices exceeds this number, older indices will be dropped.
# elasticsearch_max_number_of_indices*elasticsearch_max_docs_per_index=total number of messages in your setup
elasticsearch_max_number_of_indices = <%= node.graylog2.max_indices %>

# How many ElasticSearch shards and replicas should be used per index? Note that this only applies to newly created indices.
elasticsearch_shards = 4
elasticsearch_replicas = 0

# Analyzer (tokenizer) to use for message and full_message field. The "standard" filter usually is a good idea.
# All supported analyzers are: standard, simple, whitespace, stop, keyword, pattern, language, snowball, custom
# ElasticSearch documentation: http://www.elasticsearch.org/guide/reference/index-modules/analysis/
# Note that this setting only takes effect on newly created indices.
elasticsearch_analyzer = standard

# How many minutes of messages do you want to keep in the recent index? This index lives in memory only and is used to build the overview and stream pages. Raise this value if you want to see more messages in the overview pages. This is not affecting for example searches which are always targeting *all* indices.
recent_index_ttl_minutes = 60

# Storage type of recent index. Allowed values: niofs, simplefs, mmapfs, memory
# Standard: niofs - Set to memory for best speed but keep in mind that the whole recent index has to fit into the memory of your ElasticSearch machines. Set recent_index_ttl_minutes to a reasonable amount that will let the messages fit into memory.
recent_index_store_type = niofs

# Always try a reverse DNS lookup instead of parsing hostname from syslog message?
force_syslog_rdns = false
# Set time to NOW if parsing date/time from syslog message failed instead of rejecting it?
allow_override_syslog_date = true

# Batch size for all outputs. This is the maximum (!) number of messages an output module will get at once.
# For example, if this is set to 5000 (default), the ElasticSearch Output will not index more than 5000 messages
# at once. After that index operation is performed, the next batch will be indexed. If there is only 1 message
# waiting, it will only index that single message. It is important to raise this parameter if you send in so
# many messages that it is not enough to index 5000 messages at once. (Only at *really* high message rates)
output_batch_size = 5000

# The number of parallel running processors.
# Raise this number if your buffers are filling up.
processbuffer_processors = <%= node.graylog2.processbuffer_processors %>
outputbuffer_processors = <%= node.graylog2.outputbuffer_processors %>

# Wait strategy describing how buffer processors wait on a cursor sequence. (default: sleeping)
# Possible types:
#  - yielding
#     Compromise between performance and CPU usage.
#  - sleeping
#     Compromise between performance and CPU usage. Latency spikes can occur after quiet periods.
#  - blocking
#     High throughput, low latency, higher CPU usage.
#  - busy_spinning
#     Avoids syscalls which could introduce latency jitter. Best when threads can be bound to specific CPU cores.
processor_wait_strategy = <%= node.graylog2.processor_wait_strategy %>

# Size of internal ring buffers. Raise this if raising outputbuffer_processors does not help anymore.
# For optimum performance your LogMessage objects in the ring buffer should fit in your CPU L3 cache.
# Start server with --statistics flag to see buffer utilization.
# Must be a power of 2. (512, 1024, 2048, ...)
ring_size = 1024


# MongoDB Configuration
mongodb_host = <%= node.graylog2.mongodb.host %>
mongodb_port = <%= node.graylog2.mongodb.port %>
mongodb_max_connections = <%= node.graylog2.mongodb.max_connections %>
mongodb_database = <%= node.graylog2.mongodb.database %>
mongodb_useauth = <%= node.graylog2.mongodb.auth %>
mongodb_user = <%= node.graylog2.mongodb.user %>
mongodb_password = <%= node.graylog2.mongodb.password %>

# Raise this according to the maximum connections your MongoDB server can handle if you encounter MongoDB connection problems.
mongodb_max_connections = 100

# Number of threads allowed to be blocked by MongoDB connections multiplier. Default: 5
# If mongodb_max_connections is 100, and mongodb_threads_allowed_to_block_multiplier is 5, then 500 threads can block. More than that and an exception will be thrown.
# http://api.mongodb.org/java/current/com/mongodb/MongoOptions.html#threadsAllowedToBlockForConnectionMultiplier
mongodb_threads_allowed_to_block_multiplier = 5

# Graylog Extended Log Format (GELF)
use_gelf = true
gelf_listen_address = 0.0.0.0
gelf_listen_port = 12201

# Drools Rule File (Use to rewrite incoming log messages)
# See: http://support.torch.sh/help/kb/graylog2-server/custom-message-rewritingprocessing
# rules_file = /etc/graylog2.d/rules/graylog2.drl

# AMQP
amqp_enabled = false
amqp_host = localhost
amqp_port = 5672
amqp_username = guest
amqp_password = guest
amqp_virtualhost = /

# HTTP input
# http://support.torch.sh/help/kb/graylog2-server/using-the-gelf-http-input
http_enabled = false
http_listen_address = 0.0.0.0
http_listen_port = 12202

# Email transport
transport_email_enabled = <%= node.graylog2.transport.email_enabled %>
transport_email_hostname = <%= node.graylog2.transport.email_hostname %>
transport_email_port = <%= node.graylog2.transport.email_port %>
transport_email_use_auth = <%= node.graylog2.transport.email_use_auth %>
transport_email_use_tls = <%= node.graylog2.transport.email_use_tls %>
transport_email_auth_username = <%= node.graylog2.transport.email_auth_username %>
transport_email_auth_password = <%= node.graylog2.transport.email_auth_password %>
transport_email_subject_prefix = <%= node.graylog2.transport.email_subject_prefix %>
transport_email_from_email = <%= node.graylog2.transport.email_from_email %>
transport_email_from_name = <%= node.graylog2.transport.email_from_name %>
transport_email_web_interface_url = <%= node.graylog2.transport.email_web_interface_url %>
# Jabber/XMPP transport
transport_jabber_enabled = false
transport_jabber_hostname = jabber.example.com
transport_jabber_port = 5222
transport_jabber_use_sasl_auth = true
transport_jabber_allow_selfsigned_certs = false
transport_jabber_auth_username = your_user
transport_jabber_auth_password = secret
transport_jabber_message_prefix = [graylog2]

# Filters
# Enable the filter that tries to extract additional fields from k=v values in the log message?
enable_tokenizer_filter = true

# Additional modules
# Graphite
enable_graphite_output = <%= node.graylog2.graphite.enabled %>
graphite_carbon_host = <%= node.graylog2.graphite.carbon_host %>
graphite_carbon_tcp_port = <%= node.graylog2.graphite.carbon_tcp_port %>
graphite_prefix = <%= node.graylog2.graphite.prefix %>

# Librato Metrics (http://support.torch.sh/help/kb/graylog2-server/using-librato-metrics-with-graylog2)
#enable_libratometrics_output = false
#enable_libratometrics_system_metrics = false
#libratometrics_api_user = you@example.com
#libratometrics_api_token = abcdefg12345
#libratometrics_prefix = gl2-
#libratometrics_interval = 60
#libratometrics_stream_filter =
#libratometrics_host_filter =

